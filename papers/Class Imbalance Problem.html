<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Class Imbalance Problem</title>
  <meta name="description" content="**">

  <link rel="stylesheet" href="Class%20Imbalance%20Problem_files/main.css">
  <link rel="canonical" href="http://www.chioka.in/class-imbalance-problem/">
  <link rel="alternate" type="application/rss+xml" title="Garbled Notes" href="http://www.chioka.in/feed.xml">

  <link rel="stylesheet" href="Class%20Imbalance%20Problem_files/font-awesome.css">
  <link href="Class%20Imbalance%20Problem_files/css_002.css" rel="stylesheet" type="text/css">
  <link href="Class%20Imbalance%20Problem_files/css.css" rel="stylesheet" type="text/css">
<script async="" src="Class%20Imbalance%20Problem_files/analytics.js"></script><script src="Class%20Imbalance%20Problem_files/embed.js" data-timestamp="1513189697142"></script><link rel="prefetch" as="style" href="Class%20Imbalance%20Problem_files/a_data/lounge.css"><link rel="prefetch" as="script" href="Class%20Imbalance%20Problem_files/a_data/common.js"><link rel="prefetch" as="script" href="Class%20Imbalance%20Problem_files/a_data/lounge_002.js"><link rel="prefetch" as="script" href="Class%20Imbalance%20Problem_files/a_data/config.js"><script src="Class%20Imbalance%20Problem_files/alfalfa.js" async="" charset="UTF-8"></script></head>


  <body>

    <div class="site-header">

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <i class="fa fa-navicon fa-lg"></i>
      </a>

      <div class="trigger">
          <a class="page-link" href="http://www.chioka.in/">Home</a>
        
          
          <a class="page-link" href="http://www.chioka.in/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>


</div>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
     <h1 class="post-title">Class Imbalance Problem</h1>
     <p class="post-meta">Posted on Aug 30, 2013 • lo</p>
  </header>

  <article class="post-content">
    <h2 id="section"><em>**</em></h2>

<h2 id="what-is-the-class-imbalance-problem">What is the Class Imbalance Problem?</h2>

<p>It is the problem in machine learning where <strong>the total number of a class of data (positive) is far less than the total number of another class of data (negative)</strong>.
 This problem is extremely common in practice and can be observed in 
various disciplines including fraud detection, anomaly detection, 
medical diagnosis, oil spillage detection, facial recognition, etc.</p>

<h2 id="why-is-it-a-problem">Why is it a problem?</h2>

<p>Most machine learning algorithms and works best when&nbsp;the number 
of instances of each classes are roughly equal. When the number of 
instances of one class far exceeds the other, problems arise. This is 
best illustrated below with an example.</p>

<p>Given a dataset of transaction data, we would like to find out which 
are fraudulent and which are genuine ones. Now, it is highly cost to the
 e-commerce company if a fraudulent transaction goes through as this 
impacts our customers trust in us, and costs us money. So we want to 
catch as many fraudulent transactions as possible.</p>

<p>If there is a dataset consisting of 10000 genuine and 10 fraudulent 
transactions, the classifier will tend to classify fraudulent 
transactions as genuine transactions. The reason can be easily explained
 by the numbers. Suppose the machine learning algorithm has two possibly
 outputs as follows:</p>

<ol>
  <li>Model 1 classified 7 out of 10 fraudulent transactions as genuine 
transactions and 10 out of 10000 genuine transactions as fraudulent 
transactions.</li>
  <li>Model 2 classified 2&nbsp;out of 10 fraudulent transactions as 
genuine transactions and 100 out of 10000 genuine transactions as 
fraudulent transactions.</li>
</ol>

<p>If the classifier’s performance is determined by the number of 
mistakes, then clearly Model 1 is better as it makes only a total of 17 
mistakes while Model 2 made 102 mistakes. However, as we want to 
minimize the number of fraudulent transactions happening, we should pick
 Model 2 instead which only made 2 mistakes classifying the fraudulent 
transactions. Of course, this could come at the expense of more genuine 
transactions being classified as fraudulent transactions, but will be a 
cost we can bear for now. Anyhow, a general machine learning algorithm 
will just pick Model 1 than Model 2, which is a problem. In practice, 
this means we will let a lot of fraudulent transactions go through 
although we could have stopped them by using Model 2. This translates to
 unhappy customers and money lost for the company.</p>

<h2 id="how-to-tell-the-machine-learning-algorithm-which-is-the-better-solution">How to tell the machine learning algorithm which is the better solution?</h2>

<p>To tell the machine learning algorithm (or the researcher) that Model
 2 is better than Model 1, we need to show that Model 2 above is better 
than Model 1 above. For that, we will need better metrics than just 
counting the number of mistakes made.</p>

<p>We introduce the concept of True Positive, True Negative, False Positive and False Negative:</p>

<ul>
  <li>True Positive (TP) – An example that is&nbsp;<span style="color: #0000ff;"><strong>positive</strong></span> and is classified correctly as <span style="color: #0000ff;"><strong>positive</strong></span></li>
  <li>True Negative (TN) – An example that is&nbsp;<span style="color: #ff0000;"><strong>negative</strong></span> and is classified correctly as&nbsp;<span style="color: #ff0000;"><strong>negative</strong></span></li>
  <li>False Positive (FP) – An example that is&nbsp;<span style="color: #ff0000;"><strong>negative</strong></span> but is classified wrongly as <span style="color: #0000ff;"><strong>positive</strong></span></li>
  <li>False Negative (FN) – An example that is&nbsp;<span style="color: #0000ff;"><strong>positive</strong></span> but is classified wrongly as <strong style="color: #000000; font-style: normal;"><span style="color: #ff0000;">negative</span></strong></li>
</ul>

<p>Based on this above. We will have also the following of True Positive
 Rate, True Negative Rate, False Positive Rate, False Negative Rate:</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/Metrics-Table.png"><img class="alignnone size-medium wp-image-97" alt="Metrics Table" src="Class%20Imbalance%20Problem_files/Metrics-Table-580x280.png" srcset="/wp-content/uploads/2013/08/Metrics-Table-580x280.png 580w, /wp-content/uploads/2013/08/Metrics-Table-940x455.png 940w, /wp-content/uploads/2013/08/Metrics-Table.png 1018w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="280"></a></p>

<p>With these new metrics, let’s compare it with the conventional 
metrics of counting the number of mistakes made with the example above. 
First, we will use the old metrics to calculate the number of mistakes 
made (error):</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/Traditional-Metrics.png"><img class="alignnone size-medium wp-image-99" alt="Traditional Metrics" src="Class%20Imbalance%20Problem_files/Traditional-Metrics-580x254.png" srcset="/wp-content/uploads/2013/08/Traditional-Metrics-580x254.png 580w, /wp-content/uploads/2013/08/Traditional-Metrics-940x413.png 940w, /wp-content/uploads/2013/08/Traditional-Metrics.png 951w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="254"></a></p>

<p>As illustrated above, Model 1 looks like it has lower error (0.1% 
error) than Model 2 (1.0% error) but we know that Model 2 is the better 
one, as it makes less false negatives (FN) (maximize true positive 
(TP)). Now let’s see what the performance of Model 1 and Model 2 are 
like with the new metrics:</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/New-Metrics.png"><img class="alignnone size-medium wp-image-100" alt="New Metrics" src="Class%20Imbalance%20Problem_files/New-Metrics-580x237.png" srcset="/wp-content/uploads/2013/08/New-Metrics-580x237.png 580w, /wp-content/uploads/2013/08/New-Metrics-940x385.png 940w, /wp-content/uploads/2013/08/New-Metrics.png 1007w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="237"></a></p>

<p>Now, we can see that the false negative rate of Model 1 is at 70% 
while the false negative rate of Model 2 is just at 20%, which is 
clearly a better classifier. This is what we should educate the machine 
learning algorithm (or us) to use in order to allow it to pick a better 
algorithm.</p>

<h2 id="how-to-mitigate-this-problem">How to mitigate this problem?</h2>

<p>Now knowing what the Class Imbalance Problem is and why is it a problem, we need to know how to deal with this problem.</p>

<p>We can roughly classify the approaches into two major categories: sampling based approaches and cost function based approaches.</p>

<h3 id="cost-function-based-approaches">Cost function based approaches</h3>

<p>The intuition behind cost function based approaches is that if we 
think one false negative is worse than one false positive, we will count
 that one false negative as, e.g., 100 false negatives instead. For 
example, if 1 false negative is as costly as 100 false positives, then 
the machine learning algorithm will try to make fewer false negatives 
compared to false positives (since it is cheaper). For example, in the 
case of SVM, the generic formula is:</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/SVM_formula.png"><img class="alignnone size-full wp-image-104 aligncenter" alt="SVM_formula" src="Class%20Imbalance%20Problem_files/SVM_formula.png" width="163" height="85"></a></p>

<p>where <strong>w</strong> is the normal vector to the hyperplane. and <strong>E[i]</strong> is the error of each data instance, <strong>C</strong> is a cost constant and <strong>n</strong>
 is the number of data instances. To assign a different cost function to
 false negative and false positive, we can modify the formula to as 
follows:</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/SVM_formula_2.png"><img class="alignnone size-full wp-image-105 aligncenter" alt="SVM_formula_2" src="Class%20Imbalance%20Problem_files/SVM_formula_2.png" width="279" height="85"></a></p>

<p>Where <strong>C+</strong>&nbsp;is a cost constant for positive cases and <strong>C-</strong> is a cost constant for negative cases, <strong>n+</strong> is the total number of positive cases and <strong>n-</strong>
 is the total number of negative cases. Without diving too deep into the
 formula above, this is just an example how one may assign different to 
cost to positive and negative classes.</p>

<h3 id="sampling-based-approaches">Sampling based approaches</h3>

<p>This can be roughly classified into three categories:</p>

<ol>
  <li>Oversampling, by adding more of the minority class so it has more effect on the machine learning algorithm</li>
  <li>Undersampling, by removing some of the majority class so it has less effect on the machine learning algorithm</li>
  <li>Hybrid, a mix of oversampling and undersampling</li>
</ol>

<p>However, these approaches have clear drawbacks, as explained below.</p>

<h4 id="undersampling">Undersampling</h4>

<p>By undersampling, we could risk removing some of the majority class 
instances which is more representative, thus discarding useful 
information. This can be illustrated as follows:&nbsp;<a href="http://www.chioka.in/wp-content/uploads/2013/08/Undersampling.png"><img class="alignnone size-medium wp-image-107" alt="Undersampling" src="Class%20Imbalance%20Problem_files/Undersampling-580x197.png" srcset="/wp-content/uploads/2013/08/Undersampling-580x197.png 580w, /wp-content/uploads/2013/08/Undersampling.png 667w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="197"></a></p>

<p>Here the green line is the ideal decision boundary we would like to 
have, and blue is the actual result. On the left side is the result of 
just applying a general machine learning algorithm without using 
undersampling. On the right, we undersampled the negative class but 
removed some informative negative class, and caused the blue decision 
boundary to be slanted, causing some negative class to be classified as 
positive class wrongly.</p>

<h4 id="oversampling">Oversampling</h4>

<p>By oversampling, just duplicating the minority classes could lead the
 classifier to overfitting to a few examples, which can be illustrated 
below:<a href="http://www.chioka.in/wp-content/uploads/2013/08/Oversampling.png"><img class="alignnone size-medium wp-image-106" alt="Oversampling" src="Class%20Imbalance%20Problem_files/Oversampling-580x197.png" srcset="/wp-content/uploads/2013/08/Oversampling-580x197.png 580w, /wp-content/uploads/2013/08/Oversampling.png 667w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="197"></a></p>

<p>On the left hand side is before oversampling, where as on the right 
hand side is oversampling has been applied. On the right side, The thick
 positive signs indicate there are multiple repeated copies of that data
 instance. The machine learning algorithm then sees these cases many 
times and thus designs to overfit to these examples specifically, 
resulting in a blue line boundary as above.</p>

<h4 id="hybrid-approach">Hybrid approach</h4>

<p>By combining undersampling and oversampling approaches, we get the 
advantages but also drawbacks of both approaches as illustrated above, 
which is still a tradeoff.</p>

<h3 id="more-recent-approaches-to-the-problem">More recent approaches to the problem</h3>

<p>In 2002, an sampling based algorithm called SMOTE (Synthetic Minority
 Over-Sampling Technique) was introduced that try to address the class 
imbalance problem. It is one of the most adopted approaches due to its 
simplicity and effectiveness. It is a combination of oversampling and 
undersampling, but the oversampling approach is not by replicating 
minority class but constructing new minority class data instance via an 
algorithm.</p>

<p>In traditional oversampling, minority class is replicated exactly. In SMOTE, new minority instances are constructed in this way:</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/SMOTE.png"><img class="alignnone size-medium wp-image-108" alt="SMOTE" src="Class%20Imbalance%20Problem_files/SMOTE-580x228.png" srcset="/wp-content/uploads/2013/08/SMOTE-580x228.png 580w, /wp-content/uploads/2013/08/SMOTE.png 887w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="228"></a></p>

<p>The intuition behind the construction algorithm is that oversampling 
causes overfit because of repeated instances causes the decision 
boundary to tighten. Instead, we will create “similar” examples instead.
 To the machine learning algorithm, these new constructed instances are 
not exact copies and thus&nbsp;softens the decision boundary as a 
result. This be can illustrated as follows:</p>

<p><a href="http://www.chioka.in/wp-content/uploads/2013/08/SMOTE-boundary.png"><img class="alignnone size-medium wp-image-109" alt="SMOTE boundary" src="Class%20Imbalance%20Problem_files/SMOTE-boundary-580x197.png" srcset="/wp-content/uploads/2013/08/SMOTE-boundary-580x197.png 580w, /wp-content/uploads/2013/08/SMOTE-boundary.png 667w" sizes="(max-width: 580px) 100vw, 580px" width="580" height="197"></a></p>

<p>As a result, the classifier is more general and does not overfit.</p>

<h3 id="even-more-recent-approaches">Even more recent approaches</h3>

<p>Interested readers may look into more recent literature regarding 
RUSBoost, SMOTEBagging and Underbagging, which are all regarded as more 
promising approaches since SMOTE.</p>

<p>However, SMOTE is still very popular due to its simplicity.</p>

<h2 id="summary">Summary</h2>

<p>The Class Imbalance Problem is a common&nbsp;problem affecting 
machine learning due to&nbsp;having disproportionate number of 
class&nbsp;instances in practice. To compare solutions, we will use 
alternative metrics (True Positive, True Negative, False Positive, False
 Negative) instead of general accuracy of counting number of mistakes.</p>

<p>Due to its prevalence, there are many approaches out there to deal 
with this problem. They can be generally classified into two major 
categories of 1) sampling based, and 2) cost function based. Sampling 
based can be broken into three major categories: a) over sampling b) 
under sampling c) hybrid of oversampling and undersampling.</p>

<p>Now armed with what, why and how, you can start dealing with real world scenarios of handling datasets with this problem.</p>

  </article>

  <div align="center">
  	<a href="#">
  	<i class="fa fa-arrow-circle-up fa-2x"></i>
  	</a>
  </div>

  <div id="disqus_thread"><iframe id="dsq-app7829" name="dsq-app7829" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1px !important; min-width: 100% !important; border: medium none !important; overflow: hidden !important; height: 2076px !important;" src="Class%20Imbalance%20Problem_files/a.html" horizontalscrolling="no" verticalscrolling="no" width="100%" frameborder="0"></iframe></div>
  <script>
      /**
       *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
       *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
       */
      /*
      var disqus_config = function () {
          this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      */
      (function() {  // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          
          s.src = '//log0.disqus.com/embed.js';
          
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="<a class="vglnk" href="https://disqus.com/?ref_noscript" rel="nofollow"><span>https</span><span>://</span><span>disqus</span><span>.</span><span>com</span><span>/?</span><span>ref</span><span>_</span><span>noscript</span></a>" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>

      </div>
    </div><iframe style="display: none;"></iframe>

    <div class="footer center">

  Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>

</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-8676829-1', 'auto');
  ga('send', 'pageview');
</script>


  

</body></html>